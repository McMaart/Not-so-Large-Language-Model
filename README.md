# Not so Large Language Model &mdash; Training Small Language Models on a Constrained Domain

This project aims to develop a small, efficient language model for generating children's
stories in simple English, inspired by the paper ["TinyStories"](https://arxiv.org/abs/2305.07759) (Eldan and Li, 2023).
We trained Transformer and different RNN models, with a total parameter count up to 8.3M.

This file is intended to provide a broad overview of how to set up and use our implementation.

## Installation and Setup
For running the files in this repository, the Python packages listed in the file `requirements.txt` are required, which 
can be conveniently installed with
```sh
pip install -r requirements.txt
```
For using acceleration with a GPU, see details at the [PyTorch website](https://pytorch.org/get-started/locally/) and 
the [implementation of FlashAttention](https://github.com/Dao-AILab/flash-attention).
For automatically evaluating the generated stories with GPT-4o (specifically: UHHGPT), an additional setup step is 
required. This step is described at the top of the file ``evaluation/uhhgpt_selenium.py``.

The dataset used in this project can be created by downloading and saving the files ``TinyStoriesV2-GPT4-train.txt`` and
``TinyStoriesV2-GPT4-valid.txt`` from [HuggingFace](https://huggingface.co/datasets/roneneldan/TinyStories/tree/main) to the ``data/`` directory and subsequently running the file 
``data/preprocess_dataset.py``.


## Overview of the Most Important Files

### Generating Stories
The functions for generating tokens for a story are located in ``generate_stories.py``. 
At the end of the file, it is possible to provide the model with a prompt. 
One can also make adjustments such as changing the temperature or the method used.
```py
if __name__ == '__main__':
    from io_utils import prompt_model
    
    model_name = "transformer_3.7M"  # Name of the model, must be located in trained_models/ 
    method = 'default'  # Choose the generation method: default, beam, beam_multinomial
    string = ''  # Choose start string
    story = prompt_model(model_name=model_name, start_str=string, length=256, temperature=0.0,
                         method=method)
    print(story)
```
### Training and Evaluating the Models

The functionalities for training the models are implemented in the file ``training.py``.
By default, a new transformer model with 8.3M parameters is trained when the file is run.
Details on possible alternative training parameters are documented in the file.

Additional files for more extensive evaluation are located in the folder ``evaluation/``.
We have included the first 50 stories generated by each model and the corresponding rating from ChatGPT in the 
``stories_and_ratings/`` directory.

### Model Implementations, Tokenization and Postprocessing
The Transformer model (including the positional encoding) is implemented in the file ``models/transformer_model.py``. 
The implementation of the RNN, LSTM and GRU models can be found in the file ``models/rnn_models.py``.
The methods for used for creating a vocabulary, tokenizing a sequence and postprocessing are implemented
in ``io_utils.py``.


## Generated Example

Below is an example of a generated story completion by our Transformer model with 8.3M parameters.
The prompt was adapted from the [TinyStories](https://arxiv.org/abs/2305.07759) paper.
For reproducibility, the story completion was generated with temperature 0.0.

**Prompt**: Alice was hungry, and wanted to bake a cake, but she didn’t have any sugar at home, so she decided to go
ask around. She started walking and met a squirrel. She asked the squirrel, "Would you

**Completion**: like to help me bake a cake?" The squirrel said, "Yes, I would love to help." 
So Alice and the squirrel went to the store and bought some sugar. They went back home 
and started to bake the cake together. Alice was so happy when she saw the cake.
She said, "Thank you, squirrel!" The squirrel said, "You’re welcome!" 
Alice and the squirrel shared the cake and had a great time. They were both very happy and full.
