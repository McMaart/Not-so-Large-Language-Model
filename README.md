# TinyStories &mdash; Getting Started

This file is intended to provide a broad overview of how to set up and use our 
[TinyStories](https://arxiv.org/abs/2305.07759) project implementation.

## Installation and Setup
For running the files in this repository, the Python packages listed in the file `requirements.txt` are required.
These can be conveniently installed with
```sh
pip install -r requirements.txt
```
For using acceleration with a GPU, see details at [PyTorch website](https://pytorch.org/get-started/locally/) and 
the [implementation of FlashAttention](https://github.com/Dao-AILab/flash-attention).
For performing the evaluation with UHHGPT, an additional setup step is required. This step is described
at the top of the file ``evaluation/uhhgpt_selenium.py``.

We have uploaded the cleaned **TinyStoriesV2** dataset to this repository (located at ``data/TinyStoriesV2``),
so no additional files outside of this repository are required. (However, the dataset files can also be manually created by
using the file ``data/preprocess_dataset.py`` and the original dataset files from 
[HuggingFace](https://huggingface.co/datasets/roneneldan/TinyStories/tree/main).)

Our trained models and the matching vocabulary can be found in the folder ``trained_models/``.

## Overview of the most important files

### Generating Stories
The functions for generating tokens for a story are located in ``generate_stories.py``. 
At the end of the file, it is possible to provide the model with a prompt. One can also make adjustments such as changing the temperature or the method used.
```py
if __name__ == '__main__':
    from io_utils import prompt_model
    
    model_name = "transformer_3.7M"  # Name of the model, must be located in trained_models/ 
    method = 'default'  # Choose the generation method: default, beam, beam_multinomial
    string = ''  # Choose start string
    story = prompt_model(model_name=model_name, start_str=string, length=256, temperature=0.0,
                         method=method)
    print(story)
```
### Training and Evaluating the Models

The functionalities for training and calculating the validation and test loss are implemented in the file 
``training.py``. By default, a new transformer model with 8.3M parameters is trained when the file is run.
Details on possible alternative training parameters are documented in the file.

Additional files for more extensive evaluation are located in the folder ``evaluation/``.
The stories generated by our models and corresponding ratings are located in the ``stories_and_ratings/`` directory.

### Model implementations, Tokenization and Postprocessing
The Transformer model (including the positional encoding) is implemented in the file ``models/transformer_model.py``. 
The implementation of the RNN, LSTM and GRU models can be found in the file ``models/rnn_models.py``.
The methods for used for creating a vocabulary, tokenizing a sequence and postprocessing are implemented
in ``io_utils.py``.

